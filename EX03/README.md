# EX03 â€“ Multilinear and Polynomial Regression using Scikit-Learn

---

## ðŸ“Œ Experiment Title  
**EX03 â€“ Multilinear Regression and Polynomial Regression**

## ðŸŽ¯ Objective  
To understand and implement:
- **Multilinear Regression** for predicting student academic performance  
- **Polynomial Regression** for modeling non-linear relationships in real-world data  

using Python and **scikit-learn**.

---

## ðŸ§° Libraries Used
- `pandas` â€“ data loading and preprocessing  
- `numpy` â€“ numerical computations  
- `matplotlib`, `seaborn` â€“ visualization  
- `scikit-learn` â€“ preprocessing, modeling, regularization, and evaluation  

---

## ðŸ”¹ Scenario 1: Multilinear Regression â€“ Student Performance Prediction

### Problem Statement  
Predict **student final exam performance** using multiple academic and socio-economic factors.

### Dataset  
- **Source:** Students Performance in Exams (Kaggle)  
- **File:** `StudentsPerformance.csv`

### Target Variable  
- **Final Exam Score**  
  - Computed as the average of:
    - Math score  
    - Reading score  
    - Writing score  

### Input Features  
- Gender  
- Race/Ethnicity  
- Parental level of education  
- Lunch type  
- Test preparation course  
- *(Simulated)* Study hours  
- *(Simulated)* Attendance percentage  
- *(Simulated)* Sleep hours  

### Steps Performed
- Loaded dataset into Pandas DataFrame  
- Created final score as target variable  
- Simulated missing behavioral features  
- Encoded categorical variables using **One-Hot Encoding**  
- Handled missing values using suitable imputation  
- Applied **StandardScaler** for feature scaling  
- Split data into training and testing sets  
- Trained **Multilinear Regression** model  
- Evaluated model using:
  - Mean Squared Error (MSE)  
  - Root Mean Squared Error (RMSE)  
  - RÂ² Score  
- Analyzed regression coefficients  
- Applied **Ridge** and **Lasso** regression for regularization  

### Visualizations
- Predicted vs Actual final scores  
- Feature coefficient magnitude comparison  
- Residual distribution plot  

### Outcome
Academic support indicators such as test preparation and parental education showed strong influence. Regularization improved coefficient stability and reduced multicollinearity effects.

---

## ðŸ”¹ Scenario 2: Polynomial Regression â€“ Auto MPG Prediction

### Problem Statement  
Predict **vehicle fuel efficiency (MPG)** based on **engine horsepower**, where the relationship is non-linear.

### Dataset  
- **Source:** Auto MPG Dataset (Kaggle / UCI)  
- **File:** `auto-mpg.csv`

### Target Variable  
- `mpg` â€“ Miles per gallon  

### Input Feature  
- `horsepower`

### Steps Performed
- Loaded dataset and inspected data types  
- Converted horsepower from object to numeric  
- Handled missing values (`"?"`) using mean imputation  
- Selected horsepower as independent variable  
- Applied **StandardScaler**  
- Generated polynomial features of degrees 2, 3, and 4  
- Split dataset into training and testing sets  
- Trained polynomial regression models  
- Evaluated models using:
  - MSE  
  - RMSE  
  - RÂ² Score  
- Compared performance across polynomial degrees  
- Applied **Ridge Regression** to reduce overfitting  

### Visualizations
- Polynomial curve fitting for different degrees  
- Training vs testing error comparison  
- Overfitting and underfitting demonstration  

### Outcome
Moderate-degree polynomial models captured non-linearity effectively. Higher-degree models showed overfitting, which was controlled using Ridge regularization.

---

## âœ… Key Learnings
- Importance of proper data cleaning and encoding  
- Role of feature scaling in regression models  
- Understanding non-linear relationships using polynomial features  
- Detecting and handling overfitting  
- Practical demonstration of the **biasâ€“variance tradeoff**  
- Impact of regularization on model stability and generalization  

---

## ðŸ“Œ Conclusion
This experiment demonstrates how regression techniques can be applied to both linear and non-linear problems, highlighting the importance of preprocessing, model selection, evaluation metrics, and regularization in real-world machine learning tasks.
